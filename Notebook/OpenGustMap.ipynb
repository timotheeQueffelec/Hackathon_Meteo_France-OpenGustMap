{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Open Gust Map "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recupération des données via API Meteo france"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geojson\n",
    "import requests\n",
    "import pandas as pd\n",
    "import geopandas as gpd #panda géo\n",
    "from geoalchemy2 import Geometry, WKTElement\n",
    "from io import StringIO #récupération fichier\n",
    "import time #calcul temps\n",
    "import os \n",
    "from tqdm import tqdm #calcul temps requête\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fichier liste des stations réseau étendue active (2100)\n",
    "path='your_path' #chemin à la donnée\n",
    "liste_stations= pd.read_csv(path, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter un 0 en début de chaîne si la longueur est de 7 caractères\n",
    "liste_stations['Id_station'] = liste_stations['Id_station'].astype(str).apply(lambda x: '0' + x if len(x) == 7 else x)\n",
    "\n",
    "# Vérifier que toutes les cellules contiennent 8 caractères\n",
    "all_cells_have_8_chars = all(liste_stations['Id_station'].apply(lambda x: len(x) == 8))\n",
    "\n",
    "print(\"Toutes les cellules contiennent 8 caractères :\", all_cells_have_8_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du fichier .env\n",
    "load_dotenv()\n",
    "\n",
    "# attribut du fichier .env\n",
    "apikey_commande = os.getenv(\"apikey_commande\", None)\n",
    "apikey_fichier = os.getenv(\"apikey_fichier\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paramètres d'appel de requête\n",
    "headers_commande = {\n",
    "    #Type de sortie de la requête (dans le response header de MF)\n",
    "    'accept': 'application/json',\n",
    "    #Clé API\n",
    "    'apikey' : apikey_commande }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paramètres d'appel de requête\n",
    "headers_commande = {\n",
    "    #Type de sortie de la requête (dans le response header de MF)\n",
    "    'accept': 'application/json',\n",
    "    #Clé API\n",
    "    'apikey' : apikey_fichier}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtre station ouest france\n",
    "liste_dep=[29,22,56,35,44,53,72,49,85,17,79,86,16,87,23,36,18,45,41,37,28,50,14,61,76,27,91,78,93,94,95,95,75,92,60,80,62]\n",
    "liste_stations['Dept'] = liste_stations.Id_station.astype('string').str.zfill(8).str[:2].astype(int)\n",
    "df=liste_stations[liste_stations.Dept.isin(liste_dep)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requête de commande"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialiser les DataFrames\n",
    "df_commandes = pd.DataFrame(columns=[\"id\", \"commande\"])\n",
    "df_commandes_erreur = pd.DataFrame(columns=[\"id\", \"commande\"])\n",
    "df_fichier_final = pd.DataFrame()\n",
    "df_fichier_erreur = pd.DataFrame(columns=[\"commande\"])\n",
    "\n",
    "# Boucle principale\n",
    "temps_total = 0\n",
    "for id_station in tqdm(df['Id_station'],desc=\"Progression des requêtes\"):\n",
    "    start_time = time.time()  # Début du chrono\n",
    "    \n",
    "    # Récupérer le numéro de commande à l'aide de l'API\n",
    "    url_commande = f\"https://public-api.meteofrance.fr/public/DPClim/v1/commande-station/horaire?id-station={id_station}&date-deb-periode=2023-11-01T00:00:00Z&date-fin-periode=2023-11-02T23:00:00Z\"\n",
    "    commande = requests.get(url_commande, headers=headers_commande)\n",
    "    \n",
    "    if commande.status_code == 202:\n",
    "        numero_commande = commande.json()['elaboreProduitAvecDemandeResponse']['return']\n",
    "\n",
    "        # Ajouter l'ID de station et le numéro de commande au DataFrame df_commandes\n",
    "        df_temp_commande = pd.DataFrame({\"id\": [id_station], \"commande\": [numero_commande]})\n",
    "        df_temp_commande.to_csv('commande_hackaton.csv', mode='a', header=False)\n",
    "        print(f\"{id_station}, ok\")\n",
    "        time.sleep(2)\n",
    "    else:\n",
    "        #Données erreurs\n",
    "        print(f\"Erreur pour la station {id_station}, Code de status {commande.status_code}\")\n",
    "        temp_df_erreur = pd.DataFrame({\"id\": [id_station],\"erreur\": [commande.status_code]})\n",
    "        time.sleep(2)\n",
    "        \n",
    "    end_time = time.time()  # Fin du chronomètre\n",
    "    temps_total += (end_time - start_time)\n",
    "    print(f\"Temps écoulé pour la station {id_station}: {end_time - start_time:.2f} secondes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requête Fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture fichier des commandes\n",
    "path='your_path' #chemin à la donnée\n",
    "df_fichier=pd.read_csv(path, header=None)\n",
    "df_fichier.columns = ['index','id','commande']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temps_total=0\n",
    "\n",
    "for num_commande in tqdm(df_fichier['commande'], desc='Progression des requêtes'):\n",
    "    url = f\"https://public-api.meteofrance.fr/public/DPClim/v1/commande/fichier?id-cmde={num_commande}\"\n",
    "    start_time = time.time()  # Début du chrono\n",
    "    \n",
    "    \n",
    "    # Faire un appel d'API\n",
    "    fichier = requests.get(url, headers=headers_fichier)\n",
    "\n",
    "    if fichier.status_code == 201:\n",
    "        # Traitement du fichier\n",
    "        data = fichier.text.replace(',', '.')\n",
    "        data_io = StringIO(data)\n",
    "        temp_df = pd.read_csv(data_io, sep=';')\n",
    "\n",
    "        # Sauvegarde des fichiers\n",
    "        temp_df.to_csv('./data/data_hackaton.csv', mode='a', header=False)\n",
    "        time.sleep(2)\n",
    "\n",
    "        print(f\"Récupération des données pour la commande {num_commande}\")\n",
    "    else:\n",
    "        print(f\"Erreur pour la commande {num_commande} , {fichier.status_code}\")\n",
    "        temp_df_erreur = pd.DataFrame({\"commande\": [num_commande]})\n",
    "        time.sleep(2)\n",
    "\n",
    "\n",
    "    end_time = time.time()  # Fin du chronomètre\n",
    "    temps_total += (end_time - start_time)\n",
    "    print(f\"Temps écoulé pour le fichier {num_commande}: {end_time - start_time:.2f} secondes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chemin à la donnée\n",
    "path='your_path'\n",
    "table = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ciaran = table.drop(table.columns[0], axis=1)\n",
    "Ciaran.columns = temp_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Recupération des coordonées des stations récupérés pour avoir le vent et les coordonées\n",
    "table = pd.merge(Ciaran, liste_stations, left_on = \"POSTE\", right_on = \"Id_station\", how = 'left')\n",
    "\n",
    "Table_work = table.groupby(\"POSTE\",as_index=False).agg({\"DXI\":\"first\",\n",
    "                                                               \"FXI\":\"max\",\n",
    "                                                               \"Altitude\":'first',\n",
    "                                                               'Longitude':'first',\n",
    "                                                               'Latitude':'first'})\n",
    "\n",
    "data = Table_work.copy()\n",
    "data= data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kringing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pykrige.rk import Krige\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from pykrige.ok3d import OrdinaryKriging3D\n",
    "from pykrige.uk3d import UniversalKriging3D\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from scipy.interpolate import griddata\n",
    "import rasterio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changement de CRS (pour être en 2154)\n",
    "data_gdf = gpd.GeoDataFrame(data, geometry=gpd.points_from_xy(data.Longitude,data.Latitude), \n",
    "                              crs='EPSG:2154').to_crs(\"EPSG:4326\")\n",
    "\n",
    "# Extraction données géo\n",
    "data_gdf['Longitude'] = data_gdf.geometry.x\n",
    "data_gdf['Latitude'] = data_gdf.geometry.y\n",
    "\n",
    "data = data_gdf.drop(columns = ['geometry']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Préparation des données\n",
    "X = data[['Longitude', 'Latitude', 'Altitude']]\n",
    "y = data[['FXI']]*3.6\n",
    "\n",
    "\n",
    "min_latitude=min(data['Latitude'])\n",
    "min_longitude=min(data['Longitude'])\n",
    "max_longitude=max(data['Longitude'])\n",
    "max_latitude=max(data['Latitude'])\n",
    "min_altitude=min(data['Altitude'])\n",
    "max_altitude=max(data['Altitude'])\n",
    "num_points=1000\n",
    "\n",
    "\n",
    "# Sélectionner les colonnes \n",
    "latitude = data['Latitude'].values\n",
    "longitude = data['Longitude'].values\n",
    "altitude = data['Altitude'].values\n",
    "FXI = data['FXI'].values * 3.6 #passage en km/h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Création du modele Ordinary Kriging (général)\n",
    "OK = OrdinaryKriging3D(longitude, latitude, altitude, FXI, variogram_model='linear',\n",
    "                     verbose=False, enable_plotting=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Défibition de la grille de prédiction\n",
    "\n",
    "longitude_range = np.linspace(min_longitude, max_longitude, num_points)\n",
    "latitude_range = np.linspace(min_latitude, max_latitude, num_points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Import du raster relief\n",
    "\n",
    "Raster = rasterio.open(\"./data/MNT_BD_ALTI_25m.tif\")\n",
    "\n",
    "# Adaptation de la grille avec ce dernier\n",
    "prediction_points = np.array(np.meshgrid(longitude_range, latitude_range)).T.reshape(-1, 2)\n",
    "coord_list = [(x, y) for x, y in prediction_points]\n",
    "\n",
    "coord_list_alti = np.array([x[0] for x in Raster.sample(coord_list) if x != np.nan])\n",
    "\n",
    "coord_list_alti_no_inf = np.where((coord_list_alti<5)&(coord_list_alti>-150), 5, coord_list_alti)\n",
    "coord_list_alti_no_inf = np.where(coord_list_alti<-100, 0, coord_list_alti)\n",
    "\n",
    "# Sauvegarde grille\n",
    "with open('alit_1000.npy', 'wb') as f:\n",
    "    np.save(f, coord_list_alti_no_inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Prédictions\n",
    "\n",
    "with open('./data/alti_1000.npy', 'rb') as f:\n",
    "    a = np.load(f)\n",
    "\n",
    "predicted_FXI = OK.execute(\"points\", prediction_points[:,0], prediction_points[:,1], a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshpahe prédictions pour la grille\n",
    "k3d3 = predicted_FXI[0].reshape(num_points,num_points).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage du krigeage (carte de chaleur)\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(k3d3, origin=\"lower\", cmap='jet', interpolation='nearest')\n",
    "plt.title(\"Krigeage 500m\")\n",
    "plt.xlabel(\"Long\")\n",
    "plt.ylabel(\"Lat\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
